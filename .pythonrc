try:
    import rlcompleter
    import readline
    readline.parse_and_bind("tab: complete")
    del readline, rlcompleter
    from dataclasses import dataclass, asdict, astuple
except: pass
from datetime import *
from enum import Enum
from functools import *
from itertools import *
from pprint import pprint, pformat
from string import *
import json, os, sys, random, re, time, math

_ppargs = {k: not v for k, v in pprint.__kwdefaults__.items() if type(v) is bool}
_b = vars(__builtins__)
sys.displayhook = lambda v: None if v is None else (_b.update(_=v), pp(v) if 'pp' in _b else pprint(v, **_ppargs))[0]


def xclip(data: ... = None):
    from subprocess import TimeoutExpired, check_output
    if not (data is None or isinstance(data, bytes) or hasattr(data, '__buffer__')):
        data = str(data).encode()
    try:
        return check_output(['xclip', '-sel', 'c', data and '-i' or '-o'], timeout=.1, input=data)
    except TimeoutExpired:
        pass

import tokenize
_LOWPREC = {
    int(getattr(tokenize, n))
    for n in 'NOTEQUAL PERCENT PERCENTEQUAL AMPER AMPEREQUAL STAR DOUBLESTAR DOUBLESTAREQUAL STAREQUAL PLUS PLUSEQUAL COMMA MINUS MINEQUAL SLASH DOUBLESLASH DOUBLESLASHEQUAL SLASHEQUAL COLON COLONEQUAL SEMI LESS LEFTSHIFT LEFTSHIFTEQUAL LESSEQUAL EQEQUAL GREATER GREATEREQUAL RIGHTSHIFT RIGHTSHIFTEQUAL AT ATEQUAL CIRCUMFLEX CIRCUMFLEXEQUAL VBAR VBAREQUAL'.split()
    if hasattr(tokenize, n)
}
_CLOSERS = {tokenize.LPAR: tokenize.RPAR, tokenize.LSQB: tokenize.RSQB, tokenize.LBRACE: tokenize.RBRACE}
_DISCARD = {tokenize.NEWLINE, tokenize.COMMENT, tokenize.NL, tokenize.ENCODING}

def ypytr(script: str):
    tokens = tokenize.generate_tokens(iter([script]).__next__)
    n = 0

    def tt(close: int, subj: "list[str]") -> "list[str]":
        r: "list[str]" = []
        subst = 0

        for tok in tokens:
            et = tok.exact_type
            if et is close:
                r.append(tok.string)
                break

            if et not in _DISCARD:
                if et in _CLOSERS.keys():
                    subtt = [tok.string]
                    subtt.extend(tt(_CLOSERS[et], subj))
                    r.extend(subtt)

                elif tokenize.NAME == et and "_" == tok.string:
                    if 1 == len(subj) and "_" == subj[0][0] and subj[0][1:].isdigit():
                        r.append(subj[0])
                    else:
                        nonlocal n
                        r.extend([f"(_{n}:=(", *subj, "))"])
                        subj = [f"_{n}"]
                        n+= 1

                elif et in _LOWPREC:
                    r.append(tok.string)
                    subst = len(r)

                elif tokenize.RARROW == et:
                    r = r[:subst] + tt(close, r[subst:])
                    break

                else: r.append(tok.string)
        return r

    return " ".join(tt(tokenize.ENDMARKER, ["_"]))

def ypy(script: str): return eval(ypytr(script))

def _ypy_excepthook(exctype: 'type[BaseException]', value: BaseException, traceback: ...):
    if isinstance(value, SyntaxError) and value.text:
        try:
            sys.displayhook(ypy(value.text))
            return
        except SyntaxError:
            pass
    sys.__excepthook__(exctype, value, traceback)

sys.excepthook = _ypy_excepthook
