try:
    import rlcompleter
    import readline
    readline.parse_and_bind("tab: complete")
    del readline, rlcompleter
    from dataclasses import dataclass, asdict, astuple
except: pass
from datetime import *
from enum import Enum
from functools import *
from itertools import *
from pprint import pprint, pformat
from string import *
import json, os, sys, random, re, time, math
sys.displayhook = lambda v: None if v is None else (dict.__setitem__ if dict == type(__builtins__) else setattr)(__builtins__, '_', v) or pprint(v, compact=True, sort_dicts=False, underscore_numbers=True)


def xclip(data: ... = None):
    from subprocess import TimeoutExpired, check_output
    if not (data is None or isinstance(data, bytes) or hasattr(data, '__buffer__')):
        data = str(data).encode()
    try:
        return check_output(['xclip', '-sel', 'c', data and '-i' or '-o'], timeout=.1, input=data)
    except TimeoutExpired:
        pass

import tokenize
_LOWPREC = {tokenize.NOTEQUAL, tokenize.PERCENT, tokenize.PERCENTEQUAL, tokenize.AMPER, tokenize.AMPEREQUAL, tokenize.STAR, tokenize.DOUBLESTAR, tokenize.DOUBLESTAREQUAL, tokenize.STAREQUAL, tokenize.PLUS, tokenize.PLUSEQUAL, tokenize.COMMA, tokenize.MINUS, tokenize.MINEQUAL, tokenize.SLASH, tokenize.DOUBLESLASH, tokenize.DOUBLESLASHEQUAL, tokenize.SLASHEQUAL, tokenize.COLON, tokenize.COLONEQUAL, tokenize.SEMI, tokenize.LESS, tokenize.LEFTSHIFT, tokenize.LEFTSHIFTEQUAL, tokenize.LESSEQUAL, tokenize.EQEQUAL, tokenize.GREATER, tokenize.GREATEREQUAL, tokenize.RIGHTSHIFT, tokenize.RIGHTSHIFTEQUAL, tokenize.AT, tokenize.ATEQUAL, tokenize.CIRCUMFLEX, tokenize.CIRCUMFLEXEQUAL, tokenize.VBAR, tokenize.VBAREQUAL}
_CLOSERS = {tokenize.LPAR: tokenize.RPAR, tokenize.LSQB: tokenize.RSQB, tokenize.LBRACE: tokenize.RBRACE}
_DISCARD = {tokenize.NEWLINE, tokenize.COMMENT, tokenize.NL, tokenize.ENCODING}

def ypytr(script: str):
    tokens = tokenize.generate_tokens(iter([script]).__next__)
    n = 0

    def tt(close: int, subj: "list[str]") -> "list[str]":
        r: "list[str]" = []
        subst = 0

        for tok in tokens:
            et = tok.exact_type
            if et is close:
                r.append(tok.string)
                break

            if et not in _DISCARD:
                if et in _CLOSERS.keys():
                    subtt = [tok.string]
                    subtt.extend(tt(_CLOSERS[et], subj))
                    r.extend(subtt)

                elif tokenize.NAME == et and "_" == tok.string:
                    if 1 == len(subj) and "_" == subj[0][0] and subj[0][1:].isdigit():
                        r.append(subj[0])
                    else:
                        nonlocal n
                        r.extend([f"(_{n}:=(", *subj, "))"])
                        subj = [f"_{n}"]
                        n+= 1

                elif et in _LOWPREC:
                    r.append(tok.string)
                    subst = len(r)

                elif tokenize.RARROW == et:
                    r = r[:subst] + tt(close, r[subst:])
                    break

                else: r.append(tok.string)
        return r

    return " ".join(tt(tokenize.ENDMARKER, ["_"]))

def ypy(script: str): return eval(ypytr(script))

def _ypy_excepthook(exctype: 'type[BaseException]', value: BaseException, traceback: ...):
    if isinstance(value, SyntaxError) and value.text:
        try:
            sys.displayhook(ypy(value.text))
            return
        except SyntaxError:
            pass
    sys.__excepthook__(exctype, value, traceback)

sys.excepthook = _ypy_excepthook
